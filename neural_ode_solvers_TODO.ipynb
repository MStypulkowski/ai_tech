{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_ode_solvers_TODO.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zjcwf1_YC0V3"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural ODE solvers\n",
        "In this notebook, we will take a look at four examples of using neural ODE solvers. We will get familiar with `torchdiffeq` library and use it to build models for:\n",
        "\n",
        "1. Linear system of ODEs with time-independent dynamic function\n",
        "2. Non-homogeneous ODE with time-dependent dynamic function\n",
        "3. MNIST classification\n",
        "4. Continuous Normalizing Flow (CNF)\n",
        "\n",
        "Have fun!\n",
        "\n"
      ],
      "metadata": {
        "id": "LHd4UeFSFITW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec4LZKyZVOcK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import get_cmap\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torchdiffeq\n",
        "from torchdiffeq import odeint_adjoint\n",
        "from torchdiffeq import odeint as standard_odeint\n",
        "\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils\n",
        "Some utils for plotting and loging. Don't bother, just run."
      ],
      "metadata": {
        "id": "zjcwf1_YC0V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "log_dir = './logs'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.mkdir(log_dir)"
      ],
      "metadata": {
        "id": "DmSDg_W90j_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_linear_plot(xs, ys, title, xlabel, ylabel, figsize=(8, 8), markers='-', markersize='10', legend_labels=None):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "\n",
        "    if isinstance(xs, list):\n",
        "        assert len(xs) == len(ys)\n",
        "        for x, y, marker, label in zip(xs, ys, markers, legend_labels):\n",
        "            plt.plot(x, y, marker, markersize=markersize, label=label)\n",
        "            plt.legend()        \n",
        "    else:\n",
        "        plt.plot(xs, ys, markers, markersize=markersize)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6nAhm4D7C1re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gif(gt, predicted, title, xlim, ylim, ts=None):\n",
        "    # predicted in the shape of (n_frames, n_points, 2)\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "    def animate(i):\n",
        "        ax.clear()\n",
        "        ax.set_xlim(xlim)\n",
        "        ax.set_ylim(ylim)\n",
        "        if ts is None:\n",
        "            line1, = ax.plot(gt[:, 0, 0], gt[:, 0, 1], '-', markersize='10', label='GT')\n",
        "            line2, = ax.plot(predicted[i, :, 0], predicted[i, :, 1], '--', markersize='10', label='Predicted')\n",
        "        else:\n",
        "            line1, = ax.plot(ts, gt, '-', markersize='10', label='GT')\n",
        "            line2, = ax.plot(ts, predicted[i, :], '--', markersize='10', label='Predicted')\n",
        "        ax.legend()\n",
        "        return line1, line2 \n",
        "            \n",
        "    ani = FuncAnimation(fig, animate, frames=len(predicted))    \n",
        "    ani.save(os.path.join(log_dir, '_'.join(title.lower().split(' ')) + '.gif'), dpi=300, writer=PillowWriter(fps=10))"
      ],
      "metadata": {
        "id": "pwVTqnkI2I6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple ODEs"
      ],
      "metadata": {
        "id": "hkD22tf3a37X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear system of ODEs - time-independent dynamic function\n",
        "First, we will play with some simple linear system of ODEs to get familiar with the `torchdiffeq` library.\n",
        "\n",
        "Consider a system of ODEs:\n",
        "\n",
        "\\begin{align}\n",
        "&x_1'(t) = a x_1(t) + b x_2(t) \\\\\n",
        "&x_2'(t) = c x_1(t) + d x_2(t) \n",
        "\\end{align}\n",
        "\n",
        "It can be written in a vectorized (horizontal) version:\n",
        "$$ x'(t) = x(t)A$$\n",
        "where $x(t) = [x_1(t), x_2(t)]$.\n",
        "\n",
        "Now, let's say that we have points from trajectory of $x(t)$. Our job is to create a neural network that will aproximate our dynamic function. We need to make 3 steps:\n",
        "1. Create dataset. You are given ground-truth gradients, so you need to integrate over them to get the position:\n",
        "$$ x(T) = x(0) + \\int_0^T x'(t) dt $$\n",
        "Hint: sum instead of integral is sufficient.\n",
        "2. Define neural network that only uses $x$ as an argument (no time dependency).\n",
        "3. Optimize it.\n",
        "\n"
      ],
      "metadata": {
        "id": "-uDXUI_1t4DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = torch.tensor([[2., 0.]]).to(device)\n",
        "ts = torch.linspace(0., 10., 1000).to(device)\n",
        "A_gt = torch.tensor([[-0.1, 2.0], [-2.0, -0.5]]).to(device)"
      ],
      "metadata": {
        "id": "O_xZEckKueAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_2d_data(x0, ts, A_gt):\n",
        "    \"\"\"\n",
        "    Generate data for every time step in ts.\n",
        "    Use ground-truth matrix A_gt.\n",
        "    \"\"\"\n",
        "    # TODO"
      ],
      "metadata": {
        "id": "WVJ3E_JaiKaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_gt = generate_2d_data(x0, ts, A_gt)\n",
        "make_linear_plot(x_gt[:, 0, 0].cpu(), x_gt[:, 0, 1].cpu(), '2D ODE', 'x1', 'x2')"
      ],
      "metadata": {
        "id": "_0YyEGfgZyUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ODEFunc(nn.Module):\n",
        "    def __init__(self, hid_dim=64):\n",
        "        super(ODEFunc, self).__init__()\n",
        "        \"\"\"\n",
        "        Define neural network:\n",
        "        Linear(2, hid_dim) -> SiLU -> Linear(hid_dim, 2)\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        \"\"\"\n",
        "        Make the forward pass, don't use t at all.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "r-mPC30GsufL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "odefunc_test = ODEFunc()\n",
        "x = torch.randn(5, 2)\n",
        "with torch.no_grad():\n",
        "    out = odefunc_test(None, x)\n",
        "print(out)\n",
        "\n",
        "# expected output:\n",
        "# tensor([[-0.4450,  0.0515],\n",
        "#         [ 0.0216,  0.2244],\n",
        "#         [-0.1507,  0.0914],\n",
        "#         [ 0.0363,  0.0892],\n",
        "#         [ 0.0484,  0.0241]])"
      ],
      "metadata": {
        "id": "zlzuPUAyvYPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "odefunc = ODEFunc().to(device)\n",
        "optimizer = torch.optim.Adam(odefunc.parameters(), lr=1e-2)\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "history = []\n",
        "for epoch in range(301):\n",
        "    \"\"\"\n",
        "    Calculate predicted values x_pred and compare them to\n",
        "    ground-truths using MSE loss function.\n",
        "    Use standard_odeint method. \n",
        "    It takes a nn.Module dynamic function, initial point, and integration times as its inputs.\n",
        "    \"\"\"\n",
        "    x_pred = # TODO\n",
        "    loss = # TODO\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        history.append(x_pred[:, 0, :].detach().cpu())\n",
        "        if epoch % 10 == 0:    \n",
        "            make_linear_plot([x_gt[:, 0, 0].cpu(), x_pred[:, 0, 0].cpu()], \n",
        "                             [x_gt[:, 0, 1].cpu(), x_pred[:, 0, 1].cpu()], \n",
        "                             f'Epoch {epoch} MSE {loss.item():.4f}', 'x1', 'x2', \n",
        "                             markers=['-', '--'], legend_labels=['GT', 'Predicted'])\n",
        "            clear_output(wait=True)"
      ],
      "metadata": {
        "id": "TGC6cb7WtZFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_gif(x_gt.cpu(), torch.stack(history).cpu(), 'Linear system of ODEs', [-1.5, 2], [-1.5, 2])"
      ],
      "metadata": {
        "id": "QQhRv-GN1xG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-homogeneous ODE - time-dependent dynamic function\n",
        "Now, let's bring another factor to the table - time. Let's consider an initial value problem of the form:\n",
        "$$ x'(t) = e^{-t} (\\sin(2t) + \\cos(2t)), $$\n",
        "$$ x(0) = 1. $$\n",
        "\n",
        "We know (using some justified guessing or Wolfram) that there is an analytical solution to this problem:\n",
        "$$ x(t) = \\frac{1}{5} e^{-t} (8 e^t + \\sin(2t) - 3 \\cos(2t)). $$\n",
        "\n",
        "Let's find out if we can create a model that learns the solution! Here are some steps that we need to follow:\n",
        "\n",
        "1. Generate ground truth data. This time we will do it using analytical solution.\n",
        "2. Create a neural network that uses both $x$ and $t$ as its arguments. We will do it using `ConcatLinear` layer.\n",
        "3. Optimize the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "sGrzdky5tzaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_1d_data(t):\n",
        "    return 1/5 * torch.exp(-t) * (8 * torch.exp(t) + torch.sin(2 * t) - 3 * torch.cos(2 * t))"
      ],
      "metadata": {
        "id": "hFhgCwAQt61r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = torch.tensor([[1.0]]).to(device)\n",
        "ts = torch.linspace(0., 5., 1000).to(device)\n",
        "\n",
        "x_gt = generate_1d_data(ts).unsqueeze(-1)\n",
        "make_linear_plot(ts.cpu(), x_gt[:, 0].cpu(), '1D ODE', 't', 'x')"
      ],
      "metadata": {
        "id": "rsaY4M5GzfiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ConcatLinear, self).__init__()\n",
        "        \"\"\"\n",
        "        Define a Linear layer that concatenates t and x at its input. \n",
        "        Don't use nonlinearities.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        \"\"\"\n",
        "        1. Expand dimensions of scalar t, so that it matches 2nd dimension of x, e.g.:\n",
        "            for x of shape (5, 2), t should be (5, 1) to enable concatenation.\n",
        "        2. Concatenate x and t.\n",
        "        3. Pass it through the Linear layer. \n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "QVBKuJSkG9jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "concat_linear_test = ConcatLinear(3, 2)\n",
        "x = torch.randn(5, 3)\n",
        "t = torch.tensor(1)\n",
        "with torch.no_grad():\n",
        "    out = concat_linear_test(t, x)\n",
        "print(out)\n",
        "\n",
        "# expected output:\n",
        "# tensor([[-0.4596, -1.2096],\n",
        "#         [ 0.1936, -0.2271],\n",
        "#         [ 0.0486,  0.4682],\n",
        "#         [-0.7598, -0.8102],\n",
        "#         [ 0.5184, -0.8245]])"
      ],
      "metadata": {
        "id": "DbyEt85n_Au4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ODEFunc(nn.Module):\n",
        "    def __init__(self, hid_dim=32):\n",
        "        super(ODEFunc, self).__init__()\n",
        "        \"\"\"\n",
        "        Define neural network using 4 * ConcatLinear layers with SiLU activations. \n",
        "        Input and output are 1D.\n",
        "        Don't use the activation after last layer.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        \"\"\"\n",
        "        Remember to use both t and x.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "KdW3l2NIFPcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "odefunc_test = ODEFunc()\n",
        "x = torch.randn(5, 1)\n",
        "t = torch.tensor(1)\n",
        "with torch.no_grad():\n",
        "    out = odefunc_test(t, x)\n",
        "print(out)\n",
        "\n",
        "# expected output:\n",
        "# tensor([[0.2415],\n",
        "#         [0.2396],\n",
        "#         [0.2399],\n",
        "#         [0.2401],\n",
        "#         [0.2431]])"
      ],
      "metadata": {
        "id": "yGf-udWRAiWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "odefunc = ODEFunc().to(device)\n",
        "optimizer = torch.optim.Adam(odefunc.parameters(), lr=1e-2)\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "history = []\n",
        "for i in range(351):\n",
        "    \"\"\"\n",
        "    Calculate x_pred and MSE loss. \n",
        "    Use standard_odeint.\n",
        "    \"\"\"\n",
        "    x_pred = # TODO\n",
        "    loss = # TODO\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        history.append(x_pred[:, 0].detach().cpu())\n",
        "        if i % 10 == 0:\n",
        "            make_linear_plot([ts.cpu(), ts.cpu()], [x_gt[:, 0].cpu(), x_pred[:, 0].cpu()], \n",
        "                             f'Epoch {i} MSE {loss.item():.4f}', 't', 'x', \n",
        "                             markers=['-', '--'], legend_labels=['GT', 'Predicted'])\n",
        "            clear_output(wait=True)"
      ],
      "metadata": {
        "id": "ksz7we1yHEU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_gif(x_gt.cpu(), torch.stack(history).cpu(), '1D ODE', [0, 5], [1, 1.9], ts=ts.cpu())"
      ],
      "metadata": {
        "id": "eGAWR_M_CD09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST classification\n",
        "Now, let's classify MNIST digits! A classic problem tackled from a different angle.\n",
        "\n",
        "We want to predict label $y=x(T)$ for image $x_0 = x(0)$. Let's define change of dynamics between input and output:\n",
        "$$ \\frac{dx(t)}{dt} = f(x(t), t, \\theta)$$\n",
        "\n",
        "Note, that dynamic function preserves the dimensionality of the input. To make the classification, we need to add one linear layer on top of it!"
      ],
      "metadata": {
        "id": "ou1ebEQwwWDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ConcatLinear, self).__init__()\n",
        "        \"\"\"\n",
        "        copy-paste from previous section\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "mo01u5cDuMsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        Implement block of Residual Network.\n",
        "        Use 2 * ConcatLinear with and SiLU activation.\n",
        "        x - - - - -\n",
        "        |         |\n",
        "        CL + SiLU |\n",
        "        |         |\n",
        "        CL        |\n",
        "        |         |\n",
        "        + <- - - - \n",
        "        |     \n",
        "        SiLU  \n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        \"\"\"\n",
        "        Implement the forward pass.\n",
        "        Don't forget to use both t and x, and add skip connection.\n",
        "        Use nonlineraity at the end.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "tQ5oklOkGsBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "resnet_test = ResNetBlock(2)\n",
        "x = torch.randn(4, 2)\n",
        "t = torch.tensor(1)\n",
        "with torch.no_grad():\n",
        "    out = resnet_test(t, x)\n",
        "print(out)\n",
        "\n",
        "# expected output:\n",
        "# tensor([[-0.2231, -0.2664],\n",
        "#         [ 0.0701, -0.2782],\n",
        "#         [-0.2701, -0.2697],\n",
        "#         [ 0.0243, -0.2676]])"
      ],
      "metadata": {
        "id": "ehzmDjhVZk4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ODEFunc(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim=64, n_resnet_blocks=2):\n",
        "        super(ODEFunc, self).__init__()\n",
        "        \"\"\"\n",
        "        Let's define our dynamic function but this time in more scalable way.\n",
        "        Implement NN as:\n",
        "        ConcatLinear -> n_resnet_blocks * ResNetBlock -> ConcatLinear\n",
        "        Use SiLU ativation everywhere but after last layer.\n",
        "        Note that out_dim == in_dim.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        \"\"\"\n",
        "        Implement forward pass.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "bNaOpPFSHbbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "odefunc_test = ODEFunc(2, hid_dim=10, n_resnet_blocks=3)\n",
        "x = torch.randn(3, 2)\n",
        "t = torch.tensor(1)\n",
        "with torch.no_grad():\n",
        "    out = odefunc_test(t, x)\n",
        "print(out)\n",
        "\n",
        "# expected output:\n",
        "# tensor([[0.2548, 0.3418],\n",
        "#         [0.4021, 0.2814],\n",
        "#         [0.3049, 0.3282]])"
      ],
      "metadata": {
        "id": "s1QKw1sgg0Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ODEBlock(nn.Module):\n",
        "    def __init__(self, odefunc, odeint=odeint_adjoint, rtol=1e-3, atol=1e-3):\n",
        "        super(ODEBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        We will use ODEBlock to wrap everything related to ODE solver.\n",
        "        \"\"\"\n",
        "        self.odefunc = odefunc\n",
        "        self.odeint = odeint\n",
        "        self.rtol = rtol\n",
        "        self.atol = atol\n",
        "        self.integration_times = torch.tensor([0., 1.])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Calculate output from self.odeint (adjoint method).\n",
        "        Return only the output at time t=1.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "zHiw8TuzHbpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "odefunc_test = ODEFunc(2, hid_dim=10, n_resnet_blocks=3)\n",
        "odeblock_test = ODEBlock(odefunc_test, odeint=standard_odeint, rtol=1e-4, atol=1e-4)\n",
        "x = torch.randn(3, 2)\n",
        "with torch.no_grad():\n",
        "    out = odeblock_test(x)\n",
        "print(out)\n",
        "\n",
        "# expected output:\n",
        "# tensor([[ 1.3297, -0.4251],\n",
        "#         [-1.9886,  0.3278],\n",
        "#         [-0.0987,  0.5344]])"
      ],
      "metadata": {
        "id": "da9tID4bjpWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ODEClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim=64, n_resnet_blocks=2, odeint_method=odeint_adjoint, rtol=1e-3, atol=1e-3):\n",
        "        super(ODEClassifier, self).__init__()\n",
        "        \"\"\"\n",
        "        Define classifier. We need three things:\n",
        "        1. ODEFunc\n",
        "        2. ODEBlock\n",
        "        3. Linear layer to get right dimensions for classification.\n",
        "        \"\"\"\n",
        "        self.odefunc = # TODO\n",
        "        self.ode_block = # TODO\n",
        "        self.fc_out = # TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Implement forward pass through ODEBlock and Linear layer.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "ZthWtKCRt52T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bsz = 2048\n",
        "\n",
        "in_dim = 28 * 28\n",
        "hid_dim = 64\n",
        "out_dim = 10\n",
        "n_resnet_blocks = 3\n",
        "\n",
        "odeint_method = odeint_adjoint\n",
        "rtol = 1e-3\n",
        "atol = 1e-3\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "n_epochs = 11"
      ],
      "metadata": {
        "id": "kK7qjj5gmf4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = MNIST('./data', train=True, download=True, transform=Compose([ToTensor(), Normalize((0.1307,), (0.3081,))]))\n",
        "data_test = MNIST('./data', train=False, download=True, transform=Compose([ToTensor(), Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "dataloader_train = DataLoader(data_train, batch_size=bsz, shuffle=True)\n",
        "dataloader_test = DataLoader(data_test, batch_size=bsz, shuffle=True)\n",
        "\n",
        "print(f'Loaded MNIST train split with {len(data_train)} samples')\n",
        "print(f'Loaded MNIST test split with {len(data_test)} samples')"
      ],
      "metadata": {
        "id": "YGiGG9UAgk8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    x, y = data_train[i]\n",
        "    plt.imshow(x[0], cmap='gray')\n",
        "    plt.title(str(y))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-kvFL5C6m1wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "model = ODEClassifier(in_dim, out_dim, hid_dim=hid_dim, \n",
        "                      n_resnet_blocks=n_resnet_blocks, odeint_method=odeint_adjoint, \n",
        "                      rtol=rtol, atol=atol).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "ce_loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "sirAjXT1YZdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    pbar = tqdm(dataloader_train, desc=f'Epoch {epoch}')\n",
        "    for x, y in pbar:\n",
        "        \"\"\"\n",
        "        Make the classification.\n",
        "        Hint: use F.one_hot().\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_postfix({'CE Loss': f'{loss.item():.4f}'})\n",
        "        \n",
        "    if epoch % 1 == 0:\n",
        "        with torch.no_grad():\n",
        "            accuracy_sum = 0\n",
        "            for x, y in tqdm(dataloader_test, desc='Test'):\n",
        "                x = x.reshape(-1, in_dim).to(device)\n",
        "                y = y.to(device)\n",
        "                logits = model(x)\n",
        "                accuracy_sum += (logits.argmax(1) == y).sum()\n",
        "            print(f'Test accuracy after {epoch} epochs {accuracy_sum / len(data_test) * 100 :.2f}%')"
      ],
      "metadata": {
        "id": "X1-B-z0UnaQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous Normalizing Flow (CNF)\n",
        "We will work on synthetic 2D moons dataset. Our goal is to create CNF that will learn the distribution of the data.\n",
        "\n",
        "Let's recall the three main components of CNF:\n",
        "1. Training:\n",
        "$$ z_0 = z_1 + \\int_1^0 f(z(t), t) dt $$\n",
        "2. Loss function:\n",
        "$$ \\log p(z_1) = \\log p(z_0) - \\int_0^1 \\text{tr} \\bigg( \\frac{f(z(t), t)}{dz(t)} \\bigg) dt $$\n",
        "3. Sampling:\n",
        "$$ z_1 = z_0 + \\int_0^1 f(z(t), t) dt $$"
      ],
      "metadata": {
        "id": "aR7SRDsQ9W3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_moons(width=1.0):\n",
        "    moon1 = [\n",
        "        [r * np.cos(a) - 2.5, r * np.sin(a) - 1.0]\n",
        "        for r in np.arange(5 - width, 5 + width, 0.1 * width)\n",
        "        for a in np.arange(0, np.pi, 0.01)\n",
        "    ]\n",
        "    moon2 = [\n",
        "        [r * np.cos(a) + 2.5, r * np.sin(a) + 1.0]\n",
        "        for r in np.arange(5 - width, 5 + width, 0.1 * width)\n",
        "        for a in np.arange(np.pi, 2 * np.pi, 0.01)\n",
        "    ]\n",
        "    points = torch.tensor(moon1 + moon2)\n",
        "    points += torch.rand(points.shape) * width\n",
        "    return points.float()"
      ],
      "metadata": {
        "id": "YNM-JiQYxU6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = generate_moons(0.5)\n",
        "data = (data - data.mean(0)) / data.std(0)\n",
        "print(data.shape)\n",
        "plt.scatter(data[:, 0], data[:, 1], s=1)"
      ],
      "metadata": {
        "id": "fMDdqHIMNq9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_logprob(z):\n",
        "\t\"\"\"\n",
        "\tLog-probability of standard Gaussian distribution.\n",
        "\t\"\"\"\n",
        "\treturn (-np.log(2 * np.pi) - 0.5 * z**2).sum(1, keepdim=True)"
      ],
      "metadata": {
        "id": "mdoh9bl9OhaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ConcatLinear, self).__init__()\n",
        "        \"\"\"\n",
        "        copy-paste from previous section\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "lnzU667URTWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        copy-paste from previous section\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "leSAUEoURaJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ODEFunc(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim=64, n_resnet_blocks=2):\n",
        "        super(ODEFunc, self).__init__()\n",
        "        \"\"\"\n",
        "        copy-paste from previous section\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, t, states):\n",
        "        \"\"\"\n",
        "        Implement forward pass of CNF.\n",
        "        states is a tuple (z, divergence). \n",
        "        We don't need the divergence as an input\n",
        "        but we need to calculate and return it for the purpose of integration.\n",
        "        We have two steps:\n",
        "        1. Calculate dz by passing through all of the layers.\n",
        "        2. Calculate -trace(df/dz) using torch.autograd.grad().\n",
        "        \"\"\"\n",
        "        z = states[0]\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            z.requires_grad_(True)\n",
        "\n",
        "            dz = # TODO\n",
        "\n",
        "            divergence = 0.\n",
        "            for i in range(z.shape[1]):\n",
        "                divergence += torch.autograd.grad(TODO, TODO, create_graph=True)[0][:, i]\n",
        "\n",
        "            return dz, -divergence.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "4DZKObG3RdiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "odefunc_test = ODEFunc(2, hid_dim=10, n_resnet_blocks=3)\n",
        "states = (torch.randn(3, 2), torch.randn(3, 1))\n",
        "t = torch.tensor(1.)\n",
        "out = odefunc_test(t, states)\n",
        "print(out[0])\n",
        "print(out[1])\n",
        "\n",
        "# expected output:\n",
        "# tensor([[0.2548, 0.3418],\n",
        "#         [0.4021, 0.2814],\n",
        "#         [0.3049, 0.3282]], grad_fn=<AddmmBackward0>)\n",
        "# tensor([[0.0231],\n",
        "#         [0.1463],\n",
        "#         [0.0511]], grad_fn=<NegBackward0>)"
      ],
      "metadata": {
        "id": "6mjZzKBmTJmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNF(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim=64, n_resnet_blocks=3, odeint=odeint_adjoint, rtol=1e-3, atol=1e-3):\n",
        "        super(CNF, self).__init__()\n",
        "        \"\"\"\n",
        "        Now, let's wrap everything into CNF class.\n",
        "        \"\"\"\n",
        "        self.odefunc = ODEFunc(in_dim, hid_dim=hid_dim, n_resnet_blocks=n_resnet_blocks)\n",
        "        self.odeint = odeint\n",
        "        self.rtol = rtol\n",
        "        self.atol = atol\n",
        "\n",
        "    def forward(self, z, dlogpz=None, integration_times=None, reverse=False):\n",
        "        \"\"\"\n",
        "        Implement forward pass for CNF\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        if dlogpz is None:\n",
        "            dlogpz = # TODO initialize with zeros\n",
        "        if integration_times is None:\n",
        "            integration_times = # TODO default integration times = [0., 1.]\n",
        "        if reverse:\n",
        "            integration_times = # TODO reverse the integration times\n",
        "        \n",
        "        states = # TODO use self.odeint pass initial states as tuple\n",
        "\n",
        "        if len(integration_times) == 2:\n",
        "            states = tuple(s[1] for s in states)\n",
        "        z, dlogpz = states\n",
        "\n",
        "        return states"
      ],
      "metadata": {
        "id": "BwCk9xt8Rz7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "cnf_test = CNF(2)\n",
        "z = torch.randn(3, 2)\n",
        "dlogpz = torch.zeros(3, 1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z, dlogpz = cnf_test(z, dlogpz=dlogpz)\n",
        "print(z)\n",
        "print(dlogpz)\n",
        "\n",
        "# expected output:\n",
        "# tensor([[ 0.3435, -0.1749],\n",
        "#         [-0.3217,  0.5172],\n",
        "#         [ 2.0135,  0.4417]])\n",
        "# tensor([[ 0.0059],\n",
        "#         [ 0.0269],\n",
        "#         [-0.0009]])"
      ],
      "metadata": {
        "id": "6wbSWcfhXMsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_dim = 2\n",
        "hid_dim = 64\n",
        "n_resnet_blocks = 3\n",
        "\n",
        "odeint_method = odeint_adjoint\n",
        "rtol = 1e-3\n",
        "atol = 1e-3\n",
        "\n",
        "lr = 1e-2\n",
        "\n",
        "n_epochs = 251\n",
        "n_test_samples = 10000"
      ],
      "metadata": {
        "id": "x5KZcKaRJjmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "model = CNF(in_dim, \n",
        "            hid_dim=hid_dim, \n",
        "            n_resnet_blocks=n_resnet_blocks, \n",
        "            odeint=odeint_method, \n",
        "            rtol=rtol, \n",
        "            atol=atol).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "icDEUgwGLkkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(data).to(device)\n",
        "for epoch in range(n_epochs):\n",
        "    \"\"\"\n",
        "    Fill the blanks in the training loop.\n",
        "    Calculate z0 and dlogpz using CNF's forward pass.\n",
        "    Calculate logpz0 using normal_logprob().\n",
        "    Calculate loss function.\n",
        "    \"\"\"\n",
        "    z1 = data + 1e-3 * torch.randn_like(data)\n",
        "\n",
        "    z0, dlogpz = # TODO\n",
        "\n",
        "    logpz0 = # TODO\n",
        "    logpz1 = # TODO\n",
        "    loss = -torch.mean(logpz1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch} Loss {loss.item():.4f}')\n",
        "    if epoch % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            z0 = torch.randn(n_test_samples, in_dim).to(device)\n",
        "            samples = model(z0, reverse=True)[0]\n",
        "            plt.figure(figsize=(5, 5))\n",
        "            plt.scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), alpha=0.2)\n",
        "            plt.axis('off')\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "FbS7L4PTOomh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    integration_times = torch.arange(0., 1.01, 0.01).to(device)\n",
        "\n",
        "    x = np.arange(-3, 3, 0.05)\n",
        "    y = np.arange(-3, 3, 0.05)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    X, Y = torch.tensor(X).float().to(device), torch.tensor(Y).float().to(device)\n",
        "    z_grid = torch.cat([X.reshape(-1, 1), Y.reshape(-1, 1)], 1)\n",
        "\n",
        "    dlogpz_grid = normal_logprob(z_grid)\n",
        "\n",
        "    z1s, dlogpz1s = model(z_grid, dlogpz=dlogpz_grid, integration_times=integration_times, reverse=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "def animate(i):\n",
        "    t = integration_times[i]\n",
        "    z1 = z1s[i].reshape(X.shape[0], X.shape[1], 2).cpu()\n",
        "    dlogpz1 = dlogpz1s[i].reshape(X.shape).cpu()\n",
        "\n",
        "    ax.clear()\n",
        "    ax.set_xlim([-3, 3])\n",
        "    ax.set_ylim([-3, 3])\n",
        "    \n",
        "    line = ax.pcolormesh(z1[:, :, 0], z1[:, :, 1], dlogpz1.exp())\n",
        "    cmap = get_cmap(None)\n",
        "    ax.set_facecolor(cmap(0.))\n",
        "    ax.get_xaxis().set_ticks([])\n",
        "    ax.get_yaxis().set_ticks([])\n",
        "\n",
        "    return line\n",
        "        \n",
        "ani = FuncAnimation(fig, animate, frames=len(integration_times))    \n",
        "ani.save(os.path.join(log_dir, 'cnf_density.gif'), dpi=300, writer=PillowWriter(fps=10))"
      ],
      "metadata": {
        "id": "gg7ybvLmNgJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LZW-J5jawDCv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}